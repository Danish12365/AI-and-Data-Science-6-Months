{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Ampliment normal distribution on data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment # 1:  why normal distribution is important in data science and data analysis\n",
    "\n",
    "The normal distribution, often called the Gaussian distribution, is crucial in data science and data analysis due to its unique properties and widespread applicability across various fields. It serves as a foundational concept for statistical analysis, machine learning, and data-driven decision-making. Here’s a detailed look at why the normal distribution is so important:\n",
    "\n",
    "### 1. **Central Limit Theorem**\n",
    "\n",
    "- **Definition:** The Central Limit Theorem (CLT) states that the distribution of the sum (or average) of a large number of independent, identically distributed variables approaches a normal distribution, regardless of the original distribution of the data. This makes the normal distribution a key player in inferential statistics.\n",
    "\n",
    "- **Implications:**\n",
    "  - **Approximation of Other Distributions:** The CLT allows the normal distribution to approximate other distributions, especially when sample sizes are large. This makes it easier to apply statistical methods that assume normality.\n",
    "  - **Sample Means:** Even if the underlying data isn't normally distributed, the sample means will approximate a normal distribution as the sample size increases, enabling the use of z-tests and t-tests for hypothesis testing.\n",
    "\n",
    "- **Example:** Suppose you're analyzing the average height of a population. Regardless of the original height distribution, the distribution of sample means will tend towards normality with enough samples.\n",
    "\n",
    "### 2. **Statistical Methods and Testing**\n",
    "\n",
    "- **Z-Scores and Standard Deviations:** The normal distribution enables the calculation of z-scores, which standardize data and allow comparison across different scales. This is vital in hypothesis testing and confidence interval estimation.\n",
    "\n",
    "- **Parametric Tests:** Many statistical tests, such as ANOVA, t-tests, and regression analysis, assume normality. These tests are robust and widely used because of the properties of the normal distribution.\n",
    "\n",
    "- **Confidence Intervals:** The concept of confidence intervals often relies on the normal distribution to estimate the range within which population parameters lie with a certain probability.\n",
    "\n",
    "### 3. **Probabilistic Interpretations**\n",
    "\n",
    "- **68-95-99.7 Rule (Empirical Rule):** In a normal distribution:\n",
    "  - Approximately 68% of data falls within one standard deviation from the mean.\n",
    "  - Approximately 95% falls within two standard deviations.\n",
    "  - Approximately 99.7% falls within three standard deviations.\n",
    "  \n",
    "  This helps in understanding data variability and setting thresholds for outliers or unusual observations.\n",
    "\n",
    "- **Example:** If you're evaluating test scores and find they are normally distributed, you can quickly assess how many students score within specific ranges using the empirical rule.\n",
    "\n",
    "### 4. **Machine Learning and Data Modeling**\n",
    "\n",
    "- **Assumptions in Algorithms:** Many machine learning algorithms, like linear regression, logistic regression, and discriminant analysis, assume that data is normally distributed. This assumption simplifies model fitting and interpretation.\n",
    "\n",
    "- **Gaussian Processes:** In advanced machine learning, Gaussian processes are used for tasks like regression and classification, leveraging the properties of the normal distribution.\n",
    "\n",
    "- **Feature Scaling and Transformation:** Techniques like standardization (scaling features to have zero mean and unit variance) rely on the properties of the normal distribution to improve model performance.\n",
    "\n",
    "### 5. **Natural Phenomena and Real-World Data**\n",
    "\n",
    "- **Natural Occurrence:** Many natural phenomena, such as heights, weights, and measurement errors, inherently follow a normal distribution due to the additive effects of many small, independent factors.\n",
    "\n",
    "- **Real-World Applications:** In fields like finance, biology, psychology, and engineering, the normal distribution models real-world data effectively, leading to insights and predictions.\n",
    "\n",
    "- **Example:** In finance, asset returns are often assumed to be normally distributed, allowing analysts to model risk and return using statistical techniques.\n",
    "\n",
    "### 6. **Simplification and Analytical Convenience**\n",
    "\n",
    "- **Mathematical Properties:** The normal distribution's mathematical properties, such as symmetry and smoothness, make it analytically convenient for deriving formulas and conducting statistical analyses.\n",
    "\n",
    "- **Predictive Modeling:** The closed-form solutions and properties of the normal distribution facilitate easier calculations of probabilities, making it a preferred choice for predictive modeling.\n",
    "\n",
    "- **Analytical Methods:** The normal distribution is analytically tractable, meaning it is easy to work with in mathematical calculations, including integration and differentiation.\n",
    "\n",
    "### 7. **Data Transformation and Normalization**\n",
    "\n",
    "- **Normalization Techniques:** Data scientists often use transformations, such as logarithmic or Box-Cox transformations, to make data more normally distributed. This enhances the effectiveness of statistical models and algorithms that assume normality.\n",
    "\n",
    "- **Feature Engineering:** Understanding the distribution of features helps in selecting the right transformations, leading to improved model performance and accuracy.\n",
    "\n",
    "### 8. **Handling Outliers and Anomalies**\n",
    "\n",
    "- **Outlier Detection:** The normal distribution helps identify outliers by assessing data points that fall beyond a certain number of standard deviations from the mean. This is essential in data cleaning and preprocessing.\n",
    "\n",
    "- **Anomaly Detection:** In anomaly detection, deviations from normal distribution patterns indicate potential anomalies or unusual events, aiding in tasks like fraud detection.\n",
    "\n",
    "### Visualization of the Normal Distribution\n",
    "\n",
    "Here's a simple Python code to visualize the normal distribution and its properties:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate data\n",
    "mu, sigma = 0, 1  # mean and standard deviation\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "y = norm.pdf(x, mu, sigma)\n",
    "\n",
    "# Plot the normal distribution\n",
    "plt.plot(x, y, label='Normal Distribution', color='blue')\n",
    "plt.fill_between(x, y, alpha=0.2)\n",
    "plt.title('Normal Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "\n",
    "# Add lines for mean and standard deviations\n",
    "plt.axvline(mu, color='k', linestyle='--', linewidth=1, label='Mean (μ)')\n",
    "plt.axvline(mu - sigma, color='r', linestyle='--', linewidth=1, label='-1 Std Dev (σ)')\n",
    "plt.axvline(mu + sigma, color='r', linestyle='--', linewidth=1, label='+1 Std Dev (σ)')\n",
    "plt.axvline(mu - 2*sigma, color='g', linestyle='--', linewidth=1, label='-2 Std Dev')\n",
    "plt.axvline(mu + 2*sigma, color='g', linestyle='--', linewidth=1, label='+2 Std Dev')\n",
    "plt.axvline(mu - 3*sigma, color='m', linestyle='--', linewidth=1, label='-3 Std Dev')\n",
    "plt.axvline(mu + 3*sigma, color='m', linestyle='--', linewidth=1, label='+3 Std Dev')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This code generates a plot of the normal distribution, illustrating the mean and standard deviations that correspond to the empirical rule (68-95-99.7 rule).\n",
    "\n",
    "![Normal Distribution](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*nQFX5uLfOFdWkqVh.png)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The normal distribution's importance in data science and data analysis lies in its:\n",
    "\n",
    "1. **Foundation for Statistical Analysis:** Serving as a cornerstone for various statistical methods and tests.\n",
    "2. **Real-World Applicability:** Modeling natural phenomena and real-world data effectively.\n",
    "3. **Theoretical Significance:** Underpinning the Central Limit Theorem and facilitating theoretical developments in statistics and machine learning.\n",
    "4. **Analytical Convenience:** Offering simplicity in mathematical operations and calculations.\n",
    "\n",
    "Understanding and leveraging the normal distribution is essential for effective data analysis, enabling data scientists to draw meaningful insights and make data-driven decisions across diverse domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment # 2: How to normalize the data,methods.\n",
    "\n",
    "Normalizing data is a crucial preprocessing step in data analysis and machine learning that helps to bring features into a common scale, improving the performance of algorithms that are sensitive to the scale of input data. Here, I'll explain different methods to normalize data, when to use each method, and how to implement them in Python with examples.\n",
    "\n",
    "### Why Normalize Data?\n",
    "\n",
    "1. **Improved Algorithm Performance:** Many algorithms, such as gradient descent-based optimization, k-means clustering, and support vector machines, perform better with normalized data.\n",
    "2. **Enhanced Interpretability:** Normalization makes it easier to interpret coefficients in models like linear regression.\n",
    "3. **Preventing Bias:** Prevents algorithms from being biased toward features with larger scales.\n",
    "\n",
    "### Common Normalization Methods\n",
    "\n",
    "1. **Min-Max Scaling**\n",
    "2. **Z-Score Normalization (Standardization)**\n",
    "3. **Robust Scaling**\n",
    "4. **MaxAbs Scaling**\n",
    "5. **Logarithmic Transformation**\n",
    "6. **Box-Cox Transformation**\n",
    "7. **Quantile Transformation**\n",
    "8. **Decimal Scaling**\n",
    "\n",
    "Let's dive into each method with detailed explanations, use cases, and Python implementations.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Min-Max Scaling\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Min-Max Scaling:** Rescales the feature to a fixed range, usually [0, 1]. It maintains the relationships between data points but may not handle outliers well.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
    "  \\]\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When you want to transform features to a specific range and when the distribution is not Gaussian. It's widely used in image processing and neural networks.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement Min-Max scaling using Python's `sklearn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 30], [20, 90], [30, 60], [80, 100]])\n",
    "\n",
    "# Min-Max Scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_minmax = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nMin-Max Scaled Data:\\n\", data_minmax)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [[ 50  30]\n",
    " [ 20  90]\n",
    " [ 30  60]\n",
    " [ 80 100]]\n",
    "\n",
    "Min-Max Scaled Data:\n",
    " [[0.6        0.        ]\n",
    " [0.         0.85714286]\n",
    " [0.2        0.42857143]\n",
    " [1.         1.        ]]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Simple and intuitive.\n",
    "  - Preserves data relationships.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Sensitive to outliers.\n",
    "  - Distorts data if outliers are present.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Z-Score Normalization (Standardization)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Z-Score Normalization:** Scales the data based on the mean and standard deviation. It centers the data around zero with a unit standard deviation.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "  \\]\n",
    "\n",
    "  Where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation.\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When data follows a Gaussian distribution and when the algorithm assumes a normal distribution (e.g., linear regression, logistic regression, and k-means clustering).\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can standardize data using Python's `sklearn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 30], [20, 90], [30, 60], [80, 100]])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nZ-Score Standardized Data:\\n\", data_standardized)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [[ 50  30]\n",
    " [ 20  90]\n",
    " [ 30  60]\n",
    " [ 80 100]]\n",
    "\n",
    "Z-Score Standardized Data:\n",
    " [[ 0.16903085 -1.18321596]\n",
    " [-1.52127766  0.50709255]\n",
    " [-0.84515425 -0.50709255]\n",
    " [ 2.19715127  1.18321596]]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Handles outliers better than Min-Max scaling.\n",
    "  - Suitable for Gaussian-distributed data.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Assumes normal distribution, which might not hold for all datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Robust Scaling\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Robust Scaling:** Centers and scales data using the median and the Interquartile Range (IQR), making it robust to outliers.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{robust}} = \\frac{X - \\text{Median}(X)}{\\text{IQR}(X)}\n",
    "  \\]\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When the data contains outliers and you want to scale features without being influenced by extreme values.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement Robust scaling using Python's `sklearn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 30], [20, 90], [30, 60], [80, 100], [200, 400]])\n",
    "\n",
    "# Robust Scaler\n",
    "scaler = RobustScaler()\n",
    "data_robust = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nRobust Scaled Data:\\n\", data_robust)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [[ 50  30]\n",
    " [ 20  90]\n",
    " [ 30  60]\n",
    " [ 80 100]\n",
    " [200 400]]\n",
    "\n",
    "Robust Scaled Data:\n",
    " [[ 0.          0.        ]\n",
    " [-0.66666667  1.        ]\n",
    " [-0.33333333  0.33333333]\n",
    " [ 0.66666667  0.46666667]\n",
    " [ 5.66666667  10.        ]]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Not influenced by outliers.\n",
    "  - Suitable for skewed data.\n",
    "\n",
    "- **Disadvantages:**\n",
    "  - Less interpretable compared to Min-Max scaling.\n",
    "  - May not capture the entire range of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. MaxAbs Scaling\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **MaxAbs Scaling:** Scales each feature by its maximum absolute value, preserving the sign and sparseness of the data. It scales each feature individually so that the maximum absolute value of each feature in the training set will be 1.0.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{maxabs}} = \\frac{X}{|X_{\\text{max}}|}\n",
    "  \\]\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When the data is already centered at zero and you want to maintain sparsity, often used in text data processing.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement MaxAbs scaling using Python's `sklearn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 30], [20, 90], [30, 60], [80, 100]])\n",
    "\n",
    "# MaxAbs Scaler\n",
    "scaler = MaxAbsScaler()\n",
    "data_maxabs = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nMaxAbs Scaled Data:\\n\", data_maxabs)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [[ 50  30]\n",
    " [ 20  90]\n",
    " [ 30  60]\n",
    " [ 80 100]]\n",
    "\n",
    "MaxAbs Scaled Data:\n",
    " [[0.625 0.3  ]\n",
    " [0.25  0.9  ]\n",
    " [0.375 0.6  ]\n",
    " [1.    1.   ]]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Preserves sparsity of data.\n",
    "  - Simple and intuitive.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Sensitive to outliers.\n",
    "  - Assumes data is centered at zero.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Logarithmic Transformation\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Logarithmic Transformation:** Transforms data by applying the natural logarithm, reducing skewness and making the distribution more normal-like.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{log}} = \\log(X + \\epsilon)\n",
    "  \\]\n",
    "\n",
    "  Where \\(\\epsilon\\) is a small constant added to avoid log of zero.\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When data is positively skewed, such as income or population data, and when you want to reduce variance.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement logarithmic transformation using Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([50, 20, 30, 80, 200])\n",
    "\n",
    "# Logarithmic Transformation\n",
    "data_log = np.log(data + 1e-9)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nLog Transformed Data:\\n\", data_log)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [ 50  20  30  \n",
    "\n",
    " 80 200]\n",
    "\n",
    "Log Transformed Data:\n",
    " [3.91202301 2.99573227 3.40119738 4.38202663 5.29831737]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Reduces skewness.\n",
    "  - Useful for data with exponential growth.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Can't handle zero or negative values directly.\n",
    "  - Interpretation can be challenging.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Box-Cox Transformation\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Box-Cox Transformation:** A family of power transformations that stabilize variance and make the data more normally distributed.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{boxcox}} = \\begin{cases} \n",
    "  \\frac{X^{\\lambda} - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\\n",
    "  \\log(X), & \\text{if } \\lambda = 0 \n",
    "  \\end{cases}\n",
    "  \\]\n",
    "\n",
    "  Where \\(\\lambda\\) is a parameter determined from the data.\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When you need a more flexible transformation to handle skewness and when data contains positive values.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement Box-Cox transformation using Python's `scipy` library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Sample data\n",
    "data = np.array([50, 20, 30, 80, 200])\n",
    "\n",
    "# Box-Cox Transformation\n",
    "data_boxcox, _ = boxcox(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nBox-Cox Transformed Data:\\n\", data_boxcox)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [ 50  20  30  80 200]\n",
    "\n",
    "Box-Cox Transformed Data:\n",
    " [ 9.68642348  7.3533777   8.5527533  10.9034888  14.19769711]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Handles various types of skewness.\n",
    "  - Makes data more normal-like.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Sensitive to zero or negative values.\n",
    "  - Requires positive data only.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Quantile Transformation\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Quantile Transformation:** Transforms data to follow a uniform or normal distribution using quantiles, preserving data relationships but changing the feature space.\n",
    "\n",
    "- **Formula:** Quantiles are used to map data values to a uniform or normal distribution.\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When you want to map data to a specific distribution and handle skewness effectively.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement Quantile transformation using Python's `sklearn` library:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[50, 30], [20, 90], [30, 60], [80, 100]])\n",
    "\n",
    "# Quantile Transformer\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "data_quantile = transformer.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nQuantile Transformed Data:\\n\", data_quantile)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [[ 50  30]\n",
    " [ 20  90]\n",
    " [ 30  60]\n",
    " [ 80 100]]\n",
    "\n",
    "Quantile Transformed Data:\n",
    " [[ 0.4307273  -0.4307273 ]\n",
    " [-1.15034938  0.4307273 ]\n",
    " [-0.4307273   0.        ]\n",
    " [ 1.15034938  1.15034938]]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Handles skewness and outliers well.\n",
    "  - Maps data to a specific distribution.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Computationally intensive.\n",
    "  - May distort feature relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Decimal Scaling\n",
    "\n",
    "#### Definition\n",
    "\n",
    "- **Decimal Scaling:** Transforms data by moving the decimal point, scaling features by a power of 10.\n",
    "\n",
    "- **Formula:**\n",
    "\n",
    "  \\[\n",
    "  X_{\\text{decimal}} = \\frac{X}{10^j}\n",
    "  \\]\n",
    "\n",
    "  Where \\(j\\) is the smallest integer such that \\( \\max(|X|) < 1\\).\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Use Cases:** When you want a simple scaling method and the data range is small.\n",
    "\n",
    "#### Implementation in Python\n",
    "\n",
    "Here's how you can implement Decimal Scaling:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([50, 20, 30, 80, 200])\n",
    "\n",
    "# Decimal Scaling\n",
    "j = np.ceil(np.log10(np.max(np.abs(data))))\n",
    "data_decimal = data / (10**j)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nDecimal Scaled Data:\\n\", data_decimal)\n",
    "```\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    " [ 50  20  30  80 200]\n",
    "\n",
    "Decimal Scaled Data:\n",
    " [0.05 0.02 0.03 0.08 0.2 ]\n",
    "```\n",
    "\n",
    "#### Advantages and Disadvantages\n",
    "\n",
    "- **Advantages:**\n",
    "  - Simple and intuitive.\n",
    "  - Maintains feature relationships.\n",
    "  \n",
    "- **Disadvantages:**\n",
    "  - Sensitive to data range.\n",
    "  - Less flexible for large ranges.\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing the Right Normalization Method\n",
    "\n",
    "The choice of normalization method depends on the data characteristics and the problem at hand:\n",
    "\n",
    "- **Min-Max Scaling:** Best for features with known bounds, sensitive to outliers.\n",
    "- **Z-Score Normalization:** Ideal for normally distributed data, robust to outliers.\n",
    "- **Robust Scaling:** Suitable for data with outliers and skewness.\n",
    "- **MaxAbs Scaling:** Preserves sparseness, good for zero-centered data.\n",
    "- **Logarithmic Transformation:** Reduces skewness, useful for positively skewed data.\n",
    "- **Box-Cox Transformation:** Makes data more normal-like, requires positive values.\n",
    "- **Quantile Transformation:** Maps data to a specific distribution, handles outliers.\n",
    "- **Decimal Scaling:** Simple and intuitive, best for small data ranges.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Normalization is a vital preprocessing step that ensures features are on a similar scale, improving the performance of various algorithms and models. The choice of normalization method should align with the data characteristics and the specific needs of the analysis or machine learning task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explain the distribution types role and how they help us on different kinds of data etc\n",
    "\n",
    "Understanding distribution types is crucial in statistics because they help us interpret data, choose appropriate statistical methods, and make informed decisions. Different types of data and analyses require the use of specific probability distributions that best fit the characteristics of the data. Here’s a comprehensive guide to the various types of distributions, their roles, and how they assist us in analyzing different kinds of data.\n",
    "\n",
    "## 1. **Normal Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Normal Distribution:** Also known as the Gaussian distribution, it is a continuous probability distribution characterized by its symmetric, bell-shaped curve. The mean, median, and mode are all located at the center of the distribution, and it is defined by two parameters: mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Symmetry:** The distribution is perfectly symmetrical around the mean.\n",
    "- **Bell Shape:** The curve is bell-shaped, tapering off equally in both directions from the mean.\n",
    "- **68-95-99.7 Rule:** Approximately 68% of the data falls within one standard deviation of the mean, 95% within two standard deviations, and 99.7% within three standard deviations.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Central Limit Theorem:** One of the most important roles of the normal distribution is its role in the Central Limit Theorem, which states that the sampling distribution of the sample mean approaches a normal distribution as the sample size becomes large, regardless of the shape of the population distribution. This makes it widely applicable in inferential statistics.\n",
    "\n",
    "- **Modeling Natural Phenomena:** Many natural phenomena, such as heights, weights, and test scores, tend to follow a normal distribution due to the aggregation of many small, independent effects.\n",
    "\n",
    "- **Statistical Methods:** Many statistical tests, such as t-tests and ANOVA, assume that the data follows a normal distribution. The normal distribution allows for the calculation of confidence intervals and p-values.\n",
    "\n",
    "- **Z-Scores:** Standard scores or z-scores are used to determine the position of a value relative to the mean, helping identify outliers and making comparisons across different datasets.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Height of Adults:** Suppose the heights of adult males are normally distributed with a mean of 175 cm and a standard deviation of 10 cm. The normal distribution helps estimate the probability of randomly selecting an adult male within a specific height range.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a normal distribution:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "mean = 0\n",
    "std_dev = 1\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = norm.pdf(x, mean, std_dev)\n",
    "\n",
    "plt.plot(x, y, label='Normal Distribution')\n",
    "plt.fill_between(x, y, alpha=0.2)\n",
    "plt.title('Normal Distribution (Mean = 0, SD = 1)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Normal Distribution](https://chartio.com/assets/9873ab5d5fdfdbecdf64a1fdfbc0ff25/images/blog/image-normal-distribution.svg)\n",
    "\n",
    "## 2. **Binomial Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Binomial Distribution:** A discrete probability distribution that models the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success (\\(p\\)). It is characterized by two parameters: the number of trials (\\(n\\)) and the probability of success (\\(p\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Discrete:** The binomial distribution is discrete, meaning it only takes integer values.\n",
    "- **Probability Mass Function (PMF):** The probability of observing exactly \\(k\\) successes in \\(n\\) trials is given by the PMF:\n",
    "\n",
    "  \\[\n",
    "  P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "  \\]\n",
    "\n",
    "  Where \\(\\binom{n}{k}\\) is the binomial coefficient.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Binary Outcomes:** The binomial distribution is ideal for modeling binary or dichotomous outcomes, such as pass/fail, success/failure, or yes/no.\n",
    "\n",
    "- **Quality Control:** Used in quality control processes to model the number of defective items in a batch or the success rate of a manufacturing process.\n",
    "\n",
    "- **Survey Analysis:** Helps determine the probability of a certain number of respondents answering \"yes\" in a survey.\n",
    "\n",
    "- **Clinical Trials:** In medical research, it models the number of patients responding to a treatment out of a fixed number of trials.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Coin Tossing:** If you flip a fair coin 10 times, the binomial distribution can model the probability of getting exactly 6 heads, where \\(n = 10\\) and \\(p = 0.5\\).\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a binomial distribution with \\(n = 10\\) and \\(p = 0.5\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "n = 10\n",
    "p = 0.5\n",
    "x = np.arange(0, n + 1)\n",
    "y = binom.pmf(x, n, p)\n",
    "\n",
    "plt.bar(x, y, label='Binomial Distribution', color='skyblue')\n",
    "plt.title('Binomial Distribution (n = 10, p = 0.5)')\n",
    "plt.xlabel('Number of Successes')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Binomial Distribution](https://chartio.com/assets/0f8d5b0e4d734e1a124cbb11c237d0b2/images/blog/image-binomial-distribution.svg)\n",
    "\n",
    "## 3. **Poisson Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Poisson Distribution:** A discrete probability distribution that models the number of events occurring in a fixed interval of time or space, given that these events happen with a known constant rate and independently of each other. It is characterized by a single parameter, \\(\\lambda\\), which is the average rate of occurrence.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Discrete:** The Poisson distribution is discrete and only takes non-negative integer values.\n",
    "- **Probability Mass Function (PMF):** The probability of observing \\(k\\) events in a given interval is:\n",
    "\n",
    "  \\[\n",
    "  P(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}\n",
    "  \\]\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Rare Events:** The Poisson distribution is suitable for modeling rare or infrequent events, such as the number of accidents at an intersection or the arrival of customers at a store.\n",
    "\n",
    "- **Queue Theory:** Used in queuing theory to model the number of arrivals at a service point, such as a bank teller or a call center.\n",
    "\n",
    "- **Biology and Ecology:** Models the number of occurrences of a species in a fixed area or the number of mutations in a strand of DNA over time.\n",
    "\n",
    "- **Telecommunications:** Used to model packet arrivals in network traffic and other telecommunication applications.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Customer Arrivals:** If a store expects an average of 3 customers per hour, the Poisson distribution can model the probability of receiving exactly 5 customers in an hour.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a Poisson distribution with \\(\\lambda = 3\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import poisson\n",
    "\n",
    "lambda_ = 3\n",
    "x = np.arange(0, 15)\n",
    "y = poisson.pmf(x, lambda_)\n",
    "\n",
    "plt.bar(x, y, label='Poisson Distribution', color='lightcoral')\n",
    "plt.title('Poisson Distribution (λ = 3)')\n",
    "plt.xlabel('Number of Events')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Poisson Distribution](https://chartio.com/assets/f62f014f95a0a44593d6ef33776af7ef/images/blog/image-poisson-distribution.svg)\n",
    "\n",
    "## 4. **Exponential Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Exponential Distribution:** A continuous probability distribution used to model the time between independent events occurring at a constant average rate. It is characterized by a single parameter, \\(\\lambda\\), which is the rate parameter.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Memoryless:** The exponential distribution has the memoryless property, meaning the probability of an event occurring in the future is independent of the past.\n",
    "\n",
    "- **Probability Density Function (PDF):**\n",
    "\n",
    "  \\[\n",
    "  f(x; \\lambda) = \\lambda e^{-\\lambda x} \\quad \\text{for } x \\geq 0\n",
    "  \\]\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Time Intervals:** The exponential distribution is used to model waiting times between events, such as time until the next customer arrives or the time until a light bulb burns out.\n",
    "\n",
    "- **Reliability Engineering:** Used in reliability analysis to model failure times of mechanical and electronic components.\n",
    "\n",
    "- **Survival Analysis:** In survival analysis, it models the time until an event, such as death or failure, occurs.\n",
    "\n",
    "- **Queue Theory:** Models the service time in queuing systems, helping to design and optimize service processes.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Light Bulb Lifespan:** If the average lifespan of a light bulb is 500 hours, the exponential distribution can model the probability of a bulb lasting less than 600 hours.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates an exponential distribution with \\(\\lambda = 0.2\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import expon\n",
    "\n",
    "lambda_ = 0.2\n",
    "x = np.linspace(0, 15, 1000)\n",
    "y = expon.pdf(x, scale=1/lambda_)\n",
    "\n",
    "plt.plot(x, y, label='Exponential Distribution', color='teal')\n",
    "plt.fill_between(x, y, alpha=0.2)\n",
    "plt.title('Exponential Distribution (λ = 0.2)')\n",
    "plt.xlabel('\n",
    "\n",
    "Time')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Exponential Distribution](https://chartio.com/assets/dfa41e65d5c79014e6c9780d9e89f8f8/images/blog/image-exponential-distribution.svg)\n",
    "\n",
    "## 5. **Uniform Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Uniform Distribution:** A probability distribution where all outcomes are equally likely within a defined interval. It can be either discrete or continuous.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Equally Likely Outcomes:** In a uniform distribution, every outcome in the interval has an equal probability of occurring.\n",
    "\n",
    "- **Probability Density Function (Continuous):**\n",
    "\n",
    "  \\[\n",
    "  f(x) = \\frac{1}{b-a} \\quad \\text{for } a \\leq x \\leq b\n",
    "  \\]\n",
    "\n",
    "  Where \\(a\\) and \\(b\\) are the minimum and maximum values of the interval.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Random Sampling:** The uniform distribution is used in random sampling and Monte Carlo simulations to generate random numbers within a specific range.\n",
    "\n",
    "- **Simulations:** In simulations, it helps model situations where each outcome is equally likely, such as simulating dice rolls or card draws.\n",
    "\n",
    "- **Random Number Generation:** Utilized in computer algorithms for generating random numbers, often as a starting point for more complex distributions.\n",
    "\n",
    "- **Inventory Control:** Models scenarios where demand is equally likely for all products within a range.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Rolling a Die:** A fair six-sided die roll is a discrete uniform distribution where each face has an equal probability of \\(\\frac{1}{6}\\).\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a continuous uniform distribution between \\(a = 0\\) and \\(b = 10\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "a = 0\n",
    "b = 10\n",
    "x = np.linspace(a, b, 1000)\n",
    "y = uniform.pdf(x, loc=a, scale=b-a)\n",
    "\n",
    "plt.plot(x, y, label='Uniform Distribution', color='purple')\n",
    "plt.fill_between(x, y, alpha=0.2)\n",
    "plt.title('Continuous Uniform Distribution (a = 0, b = 10)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Uniform Distribution](https://chartio.com/assets/ee5c80db4e46a4d3a2285b4410d1c1e2/images/blog/image-uniform-distribution.svg)\n",
    "\n",
    "## 6. **Log-Normal Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Log-Normal Distribution:** A continuous probability distribution of a random variable whose logarithm is normally distributed. If \\(X\\) is log-normally distributed, then \\(Y = \\log(X)\\) follows a normal distribution.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Skewed Right:** Unlike the normal distribution, the log-normal distribution is right-skewed, meaning it has a long tail on the right side.\n",
    "\n",
    "- **Probability Density Function (PDF):**\n",
    "\n",
    "  \\[\n",
    "  f(x; \\mu, \\sigma) = \\frac{1}{x \\sigma \\sqrt{2\\pi}} e^{-\\frac{(\\log x - \\mu)^2}{2\\sigma^2}} \\quad \\text{for } x > 0\n",
    "  \\]\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Multiplicative Processes:** The log-normal distribution is suitable for modeling data that result from the multiplication of many small, independent positive factors, such as stock prices or biological growth processes.\n",
    "\n",
    "- **Finance:** Used to model asset returns and stock prices, as prices cannot be negative and often exhibit multiplicative growth.\n",
    "\n",
    "- **Environmental Studies:** Models phenomena such as pollutant concentrations and rainfall amounts, which are often multiplicative.\n",
    "\n",
    "- **Income Distribution:** Frequently used in economics to model income distribution, where many small factors contribute to variations.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Stock Prices:** If the logarithm of daily stock returns is normally distributed, then the stock prices are log-normally distributed.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a log-normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 0.5\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import lognorm\n",
    "\n",
    "mu = 0\n",
    "sigma = 0.5\n",
    "x = np.linspace(0, 5, 1000)\n",
    "y = lognorm.pdf(x, s=sigma, scale=np.exp(mu))\n",
    "\n",
    "plt.plot(x, y, label='Log-Normal Distribution', color='orange')\n",
    "plt.fill_between(x, y, alpha=0.2)\n",
    "plt.title('Log-Normal Distribution (μ = 0, σ = 0.5)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Log-Normal Distribution](https://chartio.com/assets/621c4e80abbe31458d0c50db7a229f79/images/blog/image-log-normal-distribution.svg)\n",
    "\n",
    "## 7. **Chi-Square Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Chi-Square Distribution:** A continuous probability distribution that arises from the sum of the squares of \\(k\\) independent standard normal random variables. It is characterized by its degrees of freedom (\\(k\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Non-Negative:** The chi-square distribution is non-negative, taking values from 0 to \\(\\infty\\).\n",
    "- **Asymmetrical:** It is positively skewed, with the skewness decreasing as degrees of freedom increase.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Goodness-of-Fit Tests:** The chi-square distribution is used in chi-square tests for goodness-of-fit, where observed and expected frequencies are compared.\n",
    "\n",
    "- **Test of Independence:** Utilized in chi-square tests of independence to examine the relationship between categorical variables in contingency tables.\n",
    "\n",
    "- **Variance Analysis:** Plays a role in tests of variances, such as the F-test, to compare variances across groups.\n",
    "\n",
    "- **Confidence Intervals:** Used to construct confidence intervals for population variance and standard deviation.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Goodness-of-Fit Test:** Suppose you have observed frequencies of different categories and want to test if they fit an expected distribution. The chi-square distribution helps determine the likelihood of observed deviations.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates chi-square distributions with different degrees of freedom (\\(k = 2\\), \\(k = 5\\), \\(k = 10\\)):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "\n",
    "x = np.linspace(0, 15, 1000)\n",
    "y1 = chi2.pdf(x, df=2)\n",
    "y2 = chi2.pdf(x, df=5)\n",
    "y3 = chi2.pdf(x, df=10)\n",
    "\n",
    "plt.plot(x, y1, label='k=2', color='green')\n",
    "plt.plot(x, y2, label='k=5', color='blue')\n",
    "plt.plot(x, y3, label='k=10', color='red')\n",
    "plt.fill_between(x, y1, alpha=0.2, color='green')\n",
    "plt.fill_between(x, y2, alpha=0.2, color='blue')\n",
    "plt.fill_between(x, y3, alpha=0.2, color='red')\n",
    "plt.title('Chi-Square Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Chi-Square Distribution](https://chartio.com/assets/07c1b8d0d98b5e0ae69f7a6d236afaae/images/blog/image-chi-square-distribution.svg)\n",
    "\n",
    "## 8. **Student's t-Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Student's t-Distribution:** A continuous probability distribution that is symmetric and bell-shaped, similar to the normal distribution but with heavier tails. It is used when the sample size is small or the population variance is unknown, characterized by degrees of freedom (\\(k\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Symmetric:** Similar to the normal distribution, it is symmetric around the mean.\n",
    "- **Heavier Tails:** The t-distribution has heavier tails than the normal distribution, providing more probability in the tails, which accounts for the uncertainty due to small sample sizes.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Small Sample Inference:** The t-distribution is essential for making inferences about population parameters when the sample size is small (\\(n < 30\\)).\n",
    "\n",
    "- **Confidence Intervals:** Used to construct confidence intervals for the population mean when the population standard deviation is unknown.\n",
    "\n",
    "- **Hypothesis Testing:** Employed in t-tests to compare means, especially when dealing with small sample sizes or unknown variances.\n",
    "\n",
    "- **Comparing Two Means:** In two-sample t-tests, it helps determine if there is a significant difference between the means of two independent groups.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Small Sample Hypothesis Test:** Suppose you have a sample of 15 exam scores and want to test if the mean score is significantly different from a known value. The t-distribution allows for hypothesis testing under these conditions.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates t-distributions with different degrees of freedom (\\(k = 2\\), \\(k = 5\\), \\(k = 30\\)), compared to the normal distribution:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import t, norm\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y_normal = norm.pdf(x, 0, 1)\n",
    "y_t2 = t.pdf(x, df=2)\n",
    "y_t5 = t.pdf(x, df=5)\n",
    "y_t30 = t.pdf(x, df=30)\n",
    "\n",
    "plt.plot(x, y_normal, label='Normal', color='black', linestyle='dashed')\n",
    "plt.plot(x, y_t2, label='t-distribution (df=2)', color='green')\n",
    "\n",
    "\n",
    "plt.plot(x, y_t5, label='t-distribution (df=5)', color='blue')\n",
    "plt.plot(x, y_t30, label='t-distribution (df=30)', color='red')\n",
    "plt.fill_between(x, y_t2, alpha=0.2, color='green')\n",
    "plt.fill_between(x, y_t5, alpha=0.2, color='blue')\n",
    "plt.fill_between(x, y_t30, alpha=0.2, color='red')\n",
    "plt.title(\"Student's t-Distribution vs. Normal Distribution\")\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![t-Distribution](https://chartio.com/assets/f2e3d3d2b95f7281e0ee6485a8de0142/images/blog/image-t-distribution.svg)\n",
    "\n",
    "## 9. **Bernoulli Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Bernoulli Distribution:** A discrete probability distribution for a random variable that takes the value 1 with probability \\(p\\) and the value 0 with probability \\(1-p\\). It represents a single trial of a binary experiment.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Binary Outcome:** The Bernoulli distribution models binary or dichotomous outcomes.\n",
    "- **Probability Mass Function (PMF):**\n",
    "\n",
    "  \\[\n",
    "  P(X = x) = \n",
    "  \\begin{cases} \n",
    "  p & \\text{if } x = 1 \\\\\n",
    "  1-p & \\text{if } x = 0 \n",
    "  \\end{cases}\n",
    "  \\]\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Binary Experiments:** The Bernoulli distribution is the foundation for modeling binary experiments, where there are only two possible outcomes, such as success/failure or yes/no.\n",
    "\n",
    "- **Building Block for Binomial Distribution:** It serves as the building block for the binomial distribution, which models the number of successes in multiple independent Bernoulli trials.\n",
    "\n",
    "- **Hypothesis Testing:** Used in hypothesis testing involving binary data, such as testing proportions or success rates.\n",
    "\n",
    "- **Machine Learning:** In classification problems, it models binary outcomes and is used in logistic regression and other classification algorithms.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Coin Toss:** A single coin toss is a Bernoulli trial with \\(p = 0.5\\), representing the probability of landing heads.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates a Bernoulli distribution with \\(p = 0.7\\):\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "p = 0.7\n",
    "x = np.array([0, 1])\n",
    "y = np.array([1-p, p])\n",
    "\n",
    "plt.bar(x, y, label='Bernoulli Distribution', color='orchid')\n",
    "plt.xticks([0, 1], ['0 (Failure)', '1 (Success)'])\n",
    "plt.title('Bernoulli Distribution (p = 0.7)')\n",
    "plt.xlabel('Outcome')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Bernoulli Distribution](https://chartio.com/assets/ae79c7a982a58f8d03dbbd0b32ea21cf/images/blog/image-bernoulli-distribution.svg)\n",
    "\n",
    "## 10. **Beta Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Beta Distribution:** A continuous probability distribution defined on the interval [0, 1], characterized by two shape parameters, \\(\\alpha\\) and \\(\\beta\\), which determine the distribution's shape.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Flexible Shape:** The beta distribution can take various shapes (uniform, bell-shaped, U-shaped, etc.) based on the parameters \\(\\alpha\\) and \\(\\beta\\).\n",
    "\n",
    "- **Probability Density Function (PDF):**\n",
    "\n",
    "  \\[\n",
    "  f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "  \\]\n",
    "\n",
    "  Where \\(B(\\alpha, \\beta)\\) is the beta function.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Proportions:** The beta distribution is ideal for modeling proportions and probabilities, such as the likelihood of success rates or market shares.\n",
    "\n",
    "- **Bayesian Inference:** Used as a prior distribution in Bayesian statistics for modeling beliefs about probabilities before observing data.\n",
    "\n",
    "- **Beta Regression:** In regression analysis, it models response variables that are proportions or probabilities.\n",
    "\n",
    "- **Reliability Analysis:** Models the reliability or failure rates of systems over time.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Success Rate Modeling:** Suppose you want to model the probability of success in a clinical trial. The beta distribution allows you to incorporate prior knowledge and update it with observed data.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates beta distributions with different \\(\\alpha\\) and \\(\\beta\\) parameters:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "\n",
    "x = np.linspace(0, 1, 1000)\n",
    "y1 = beta.pdf(x, a=2, b=2)\n",
    "y2 = beta.pdf(x, a=5, b=2)\n",
    "y3 = beta.pdf(x, a=2, b=5)\n",
    "\n",
    "plt.plot(x, y1, label='α=2, β=2', color='blue')\n",
    "plt.plot(x, y2, label='α=5, β=2', color='green')\n",
    "plt.plot(x, y3, label='α=2, β=5', color='red')\n",
    "plt.fill_between(x, y1, alpha=0.2, color='blue')\n",
    "plt.fill_between(x, y2, alpha=0.2, color='green')\n",
    "plt.fill_between(x, y3, alpha=0.2, color='red')\n",
    "plt.title('Beta Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Beta Distribution](https://chartio.com/assets/0481e15c485c05ed7e9f93b222da6d38/images/blog/image-beta-distribution.svg)\n",
    "\n",
    "## 11. **Gamma Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Gamma Distribution:** A continuous probability distribution that models the time until \\(k\\) independent events occur, each occurring at a constant rate. It is characterized by two parameters: shape (\\(k\\)) and scale (\\(\\theta\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Positive Values:** The gamma distribution only takes positive values and is often right-skewed.\n",
    "- **Probability Density Function (PDF):**\n",
    "\n",
    "  \\[\n",
    "  f(x; k, \\theta) = \\frac{x^{k-1} e^{-x/\\theta}}{\\theta^k \\Gamma(k)} \\quad \\text{for } x > 0\n",
    "  \\]\n",
    "\n",
    "  Where \\(\\Gamma(k)\\) is the gamma function.\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Modeling Waiting Times:** The gamma distribution is used to model waiting times until multiple events occur, such as the time until failure of a system with multiple components.\n",
    "\n",
    "- **Insurance and Finance:** Used in actuarial science to model claim sizes and insurance payouts.\n",
    "\n",
    "- **Weather Modeling:** Models rainfall amounts and the duration of rainfall events.\n",
    "\n",
    "- **Reliability Engineering:** In reliability engineering, it models life data and failure rates.\n",
    "\n",
    "### Example\n",
    "\n",
    "- **Time Until Machine Failure:** Suppose a machine has multiple components, and you want to model the time until all components fail. The gamma distribution helps estimate the probability of failure over time.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "The following plot illustrates gamma distributions with different shape and scale parameters:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gamma\n",
    "\n",
    "x = np.linspace(0, 20, 1000)\n",
    "y1 = gamma.pdf(x, a=2, scale=1)\n",
    "y2 = gamma.pdf(x, a=5, scale=1)\n",
    "y3 = gamma.pdf(x, a=2, scale=2)\n",
    "\n",
    "plt.plot(x, y1, label='k=2, θ=1', color='blue')\n",
    "plt.plot(x, y2, label='k=5, θ=1', color='green')\n",
    "plt.plot(x, y3, label='k=2, θ=2', color='red')\n",
    "plt.fill_between(x, y1, alpha=0.2, color='blue')\n",
    "plt.fill_between(x, y2, alpha=0.2, color='green')\n",
    "plt.fill_between(x, y3, alpha=0.2, color='red')\n",
    "plt.title('Gamma Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Gamma Distribution](https://chartio.com/assets/720c345efcd0cd9d3a5cb91a28e7f8e8/images/blog/image-gamma-distribution.svg)\n",
    "\n",
    "## 12. **Weibull Distribution**\n",
    "\n",
    "### Definition\n",
    "\n",
    "- **Weibull Distribution:** A continuous probability distribution used to model the time to failure of systems and components. It is characterized by two parameters: shape (\\(k\\)) and scale (\\(\\lambda\\)).\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Flexible Shape:** The Weibull distribution can take various shapes (exponential, normal, etc.) based on the parameters \\(\\alpha\\) and \\(\\beta\\).\n",
    "\n",
    "- **Probability Density Function (PDF):**\n",
    "\n",
    "  \\[\n",
    "  f(x; k, \\lambda) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} e^{-(x/\\lambda)^k} \\quad \\text{for } x \\geq 0\n",
    "  \\]\n",
    "\n",
    "### Role and Applications\n",
    "\n",
    "- **Reliability Analysis:** The Weibull distribution is widely used in reliability engineering to model the life data of products and systems.\n",
    "\n",
    "- **Failure Time Modeling:** Models the time to failure of mechanical and electronic components, allowing for predictions about system reliability.\n",
    "\n",
    "- **Survival Analysis:** Used in survival analysis to model time-to-event data, such as time until"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
